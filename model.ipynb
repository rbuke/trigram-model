{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first method will use simple counting i.e. count the number of times each character is predicted based on two characters provided e.g. the name jane would defines n as the next characters after 'ja' so that would add to 'n' probability of being the next character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "%pip install torch torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, we are only looking at three characters. Two input characters, and the character we are predicting next.\n",
    "\n",
    "e.g. what characters are likely to follow r\n",
    "\n",
    "we also now what names are likely to start and finish at\n",
    "\n",
    "\n",
    "We then do a simple count of most prominent trigrams\n",
    "\n",
    "How many examples do we get from emma?\n",
    "\n",
    "<S>e -> m\n",
    "em -> m\n",
    "mm -> a\n",
    "ma -> <E>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] # hallucinate a start character and end character \n",
    "    for pair, ch2 in zip(zip(chs, chs[1:]), chs[2:]):\n",
    "        trigram  = (''.join(pair), ch2)\n",
    "        t[trigram] = t.get(trigram, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get the count of each combo and sort. we'll see that ah followed by ending character occurs the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_t = sorted(t.items(), key = lambda kv: -kv[1])\n",
    "sorted_t[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a lookup of integers to pairs so we can have an index. \n",
    "Basically we need an integer ref for all our points in our torch, that means each character pair needs an integer value, and each character needs a value e.g. ac = [100] and if it predicts d as next letter it would be axis point [100, 3]. Therefore each time this is shown, we add +1, allowing us to count the number of occurences \n",
    "\n",
    "If I understand this correctly, we therefore need two integer lookups: \n",
    "-\n",
    "Taking our char pairs as the 'y' axis - we need 0:len(unique-pairs)  i.e. 0-600 and then our single character axis being x axis, we need 28 chars (alphabet including <S> and <E>)\n",
    "\n",
    "0   |------------------------------| 28\n",
    "    |<S>aa   <S>ab  <S>ac\n",
    "    |aaa     aab    aac\n",
    "    |..\n",
    "    |..\n",
    "    |..\n",
    "784 |zaa zab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the character string to integer index for our 'y-axis' which is a - z and our <S> and <E> characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns all the set of lowercase characters\n",
    "chars = sorted(set(''.join(words)))\n",
    "char_to_integer = {s:i for i,s in enumerate(chars)}\n",
    "char_to_integer['<S>'] = 26\n",
    "char_to_integer['<E>'] = 27\n",
    "print(list(char_to_integer)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert our trigram to an array. We need to deduce how many rows we have, so it should be all the combinations of characters against our list of 28 chars.\n",
    "\n",
    "28 characters consist of 26 alphabet characters plus our <S> and <E> characters. So as we can pair same characters together e.g. -> a,a and with the other 27 chars, there is a total of 28*28 combinations -> 784 (minus some impossible combos e.g. <S><E> a<S> <E>s)\n",
    "\n",
    "Note we can remove all pairs that begin with <E> as its not possible to start on an end character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Create a list of all lowercase letters plus <S> and <E>\n",
    "letters = list(string.ascii_lowercase) + [\"<S>\", \"<E>\"]\n",
    "\n",
    "# Create a list of tuples where the first element is a pair of characters and the second element is an integer index\n",
    "pairs_string_to_integer = [(a+b, i) for i, (a, b) in enumerate((x, y) for x in letters for y in letters)]\n",
    "\n",
    "## And finally convert this to dictionary so we can do lookups\n",
    "pairs_string_to_integer = dict(pairs_string_to_integer)\n",
    "pairs_string_to_integer['fa']\n",
    "\n",
    "# Remove all keys where <S> is the second character\n",
    "pairs_string_to_integer = {key: value for key, value in pairs_string_to_integer.items() if not key.endswith('<S>')}\n",
    "\n",
    "# Remove all keys where <E> is the first character\n",
    "pairs_string_to_integer = {key: value for key, value in pairs_string_to_integer.items() if not key.startswith('<E>')}\n",
    "\n",
    "# Remove <S><E> as not possible\n",
    "pairs_string_to_integer = {key: value for key, value in pairs_string_to_integer.items() if not key == (\"<S><E>\")}\n",
    "\n",
    "# Reset the indices\n",
    "pairs_string_to_integer = {key: i for i, (key, value) in enumerate(pairs_string_to_integer.items())}\n",
    "\n",
    "print(pairs_string_to_integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates a 784 * 28 tensor that is empty initially\n",
    "import torch\n",
    "\n",
    "N = torch.zeros((len(pairs_string_to_integer), 28), dtype=torch.int32) # 28*28 = 784 for different pair combos\n",
    "N[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add our counts of each character to the tensor i.e. all counts of aab -> zzz occuring\n",
    "\n",
    "Each time e.g. ab pair with pred c comes up, we add a count to the co-ordinates in the tensor. As a result, we get the probability of each pair and next character occuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] # hallucinate a start character and end character \n",
    "    for pair, ch2 in zip(zip(chs, chs[1:]), chs[2:]):\n",
    "        trigram  = (''.join(pair), ch2) #  e.g. ('<S>e', 'm')\n",
    "        pairs_combined = trigram[0] #  e.g '<S>e'\n",
    "        print\n",
    "        ix1 = pairs_string_to_integer[pairs_combined]\n",
    "        ix2 = char_to_integer[ch2]\n",
    "        N[ix1, ix2] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverse of string to integer, this way we can look up our characters based on the cordinates in our tensor N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_itos = {i:s for s,i in char_to_integer.items()}\n",
    "pair_itos = {i:s for s,i in pairs_string_to_integer.items()}\n",
    "pair_itos[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FIX THIS\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(len(pair_itos)):\n",
    "    for j in range(len(char_itos)):\n",
    "        chstr = pair_itos[i] + char_itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the counts to proabilities of what character a pair will predict next. We add 0.01 to avoid predicting a pair which was 0 occurrences, that would result in / 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+0.01).float()  \n",
    "P /= P.sum(1, keepdims=True)\n",
    "P[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sampling we will use torch.multinomial and a generator object to make everything deterministic \n",
    "\n",
    "This line of code is using PyTorch, a popular machine learning library in Python. \n",
    "\n",
    "The `torch.Generator()` is an object that holds the state of the random number generator. You can think of it as a container for the algorithm that produces pseudo-random numbers.\n",
    "\n",
    "The `manual_seed()` function is used to set the seed for generating random numbers. This ensures that the random numbers generated are deterministic, meaning if you use the same seed, you will get the same sequence of random numbers. This is useful for debugging and testing purposes, as it allows for reproducibility in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic -> We predict the 3rd character from the beginning two e.g. em -> a. \n",
    "We then strip the first character and append our 3rd character for the next prediction i.e. ma -> b\n",
    "\n",
    "- Starting off the prediction, we need to start on <S>(x) character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_chars = []\n",
    "for key, value in pair_itos.items():\n",
    "    if '<S>' in str(value):\n",
    "        start_chars.append((key, value))\n",
    "start_chars[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - When we run through predictions, we need to predict like ab -> c -> bc -> d -> cd -> e\n",
    "Therefore we need to combine the second character in the pair with the predicted value. This is demonstrated below.\n",
    "\n",
    "Note: -1 gets the last character from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_pair = pair_itos[10][-1] + char_itos[10] \n",
    "ix_pair\n",
    "\n",
    "pairs_string_to_integer[ix_pair]\n",
    "P[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "g = torch.Generator().manual_seed(214748347)\n",
    "\n",
    "for i in range(10):\n",
    "    # Select a random value from start_chars\n",
    "    out = []\n",
    "    start_ix = random.choice(start_chars) # Gets all pairs that start with <S> \n",
    "    ix_pair = start_ix[0] # Gets integer reference for pair e.g. fe -> 45\n",
    "    out.append(start_ix[1]) # Append the first start characters\n",
    "    while True:\n",
    "        p = P[ix_pair]\n",
    "        iy_pred_reference = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(char_itos[iy_pred_reference])\n",
    "        if '<E>' in char_itos[iy_pred_reference]:\n",
    "            break\n",
    "        # Combines last letter of pair and predicted char e.g. xy predicts z  so ix_pair now equals  = yz \n",
    "        # if '<S>' in pair_itos[ix_pair]:\n",
    "\n",
    "        ix_pair_str = pair_itos[ix_pair][-1] + char_itos[iy_pred_reference] \n",
    "        \n",
    "        # Convert ix back to an integer reference\n",
    "        ix_pair = pairs_string_to_integer[ix_pair_str]\n",
    "\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at calculating the loss function to check the quality of the model\n",
    "\n",
    "GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
    "equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "equivalent to minimizing the negative log likelihood\n",
    "equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "log(a*b*c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0 \n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] # hallucinate a start character and end character \n",
    "    for pair, ch in zip(zip(chs, chs[1:]), chs[2:]):\n",
    "        pair_combined = pair[0] + pair[1]\n",
    "        ix1 = pairs_string_to_integer[pair_combined]\n",
    "        ix2 = char_to_integer[ch]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n +=1 \n",
    "        # print(f'{pair}{ch}: {prob:.4f} {logprob:.4f}')\n",
    "print(f'{log_likelihood}')\n",
    "\n",
    "negative_log_liklihood = -log_likelihood\n",
    "print(f'{negative_log_liklihood=}')\n",
    "print(f'{negative_log_liklihood/n}') # Average log likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2  - Neural Net and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now do this via a neural net - first we take the first name 'emma' as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set of all the trigrams (ab,c)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] # hallucinate a start character and end character \n",
    "    for pair, ch in zip(zip(chs, chs[1:]), chs[2:]):\n",
    "        pair_combined = pair[0] + pair[1]\n",
    "        ix1 = pairs_string_to_integer[pair_combined]\n",
    "        ix2 = char_to_integer[ch]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([706, 120, 336, 324]) = <S>e <em> <mm> <ma>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_itos[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys\n",
    "len(pairs_string_to_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=len(pairs_string_to_integer)).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the different pairs from 'emma' denoted in the below graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xenc, aspect=20/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xs)\n",
    "len(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neuron, still focusing just on emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "W = torch.randn(len(pairs_string_to_integer),28)\n",
    "\n",
    "# Multiple encodings by weight, feed all our inputs into the neuron i.e. w \n",
    "xenc @ W # 4 * 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we are getting ->  (4, 728) @ [728, 4] -> (4, 27).\n",
    "\n",
    "The above is telling, for every one of 27 neurons created, what is the firing rate of each neuron, on every one of those 4 examples. Below, (xenc @ W)[3,13] is telling us the firing rate of the 13th neuron looking at the 3rd input. \n",
    "\n",
    "This is achieved by multipled 3rd input (xs) by column 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets get exponent to make it everything positive. We can interpret these as counts in the neural net. \n",
    "# Log-counts - logics\n",
    "logits = xenc @ W # log-counts\n",
    "\n",
    "counts = logits.exp() # equivalent N\n",
    "\n",
    "# normalise to get the probs\n",
    "# counts divided by total of each row gives us the prob\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have we achieved? for every one of our 4 examples, we now have a row that came out of a neural net. And because we transformed, they are now probabilities.\n",
    "\n",
    "-- For all the operations we conducted,  interpret logits to be log counts, we exponentiate to get something that looks like log counts, then normalise to get probability. All of these are differential operations ( we can get derivative), meaning we can back propogate through and get out prob distributions\n",
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent N\n",
    "probs = counts / counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = torch.zeros(4)\n",
    "for i in range(4):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {pair_itos[x]}{char_itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------- Optimization -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((len(pairs_string_to_integer),28), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=len(pairs_string_to_integer)).float()\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'\n",
    "loss = -probs[torch.arange(4), ys].log().mean()\n",
    "loss\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass\n",
    "W.grad = None # reset gradiants\n",
    "# loss.backward backpropogates through probs and counts and logits to give gradients for our original tensir\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad[706]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform gradient descent manuallt below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad\n",
    "W.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------- !!! OPTIMIZATION !!! yay, but this time actually --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set of all the trigrams (ab,c)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] # hallucinate a start character and end character \n",
    "    for pair, ch in zip(zip(chs, chs[1:]), chs[2:]):\n",
    "        pair_combined = pair[0] + pair[1]\n",
    "        ix1 = pairs_string_to_integer[pair_combined]\n",
    "        ix2 = char_to_integer[ch]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "\n",
    "# # initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((len(pairs_string_to_integer),28), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "for k in range(140):\n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=len(pairs_string_to_integer)).float()\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  # btw: the last 2 lines here are together called a 'softmax'\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  start_ix = random.choice(start_chars) # Gets all pairs that start with <S> \n",
    "  ix_pair = start_ix[0] # Gets integer reference for pair e.g. fe -> 45\n",
    "  out.append(start_ix[1]) # Append the first start characters\n",
    "  while True:\n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix_pair]), num_classes=len(pairs_string_to_integer)).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # hack -> Set the probability of <S> to zero\n",
    "    s_index = char_to_integer['<S>']\n",
    "    p[0, s_index] = 0  # TODO: make this cleaner\n",
    "    p = p / p.sum(1, keepdims=True)  # re-normalize the probabilities\n",
    "    # ----------\n",
    "    iy_pred_reference = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(char_itos[iy_pred_reference])\n",
    "\n",
    "    if '<E>' in char_itos[iy_pred_reference]:\n",
    "      break\n",
    "    # Combines last letter of pair and predicted char e.g. xy predicts z  so ix_pair now equals  = yz \n",
    "    ix_pair_str = pair_itos[ix_pair][-1] + char_itos[iy_pred_reference] \n",
    "    # Convert ix back to an integer reference\n",
    "    ix_pair = pairs_string_to_integer[ix_pair_str]\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
